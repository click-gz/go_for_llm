{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f89cfcb",
   "metadata": {},
   "source": [
    "LLM 实际上并不是重复预测下一个单词，而是重复预测下一个 token 。对于一个句子，语言模型会先使用分词器将其拆分为一个个 token ，而不是原始的单词。对于生僻词，可能会拆分为多个 token 。\n",
    "这样可以大幅降低字典规模，提高模型训练和推断的效率。例如，对于 \"Learning new things is fun!\" 这句话，每个单词都被转换为一个 token ，而对于较少使用的单词，如 \"Prompting as powerful developer tool\"，单词 \"prompting\" 会被拆分为三个 token，即\"prom\"、\"pt\"和\"ing\"。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7603403",
   "metadata": {},
   "source": [
    "# 三种不同粒度的分词方法\n",
    "+ word-level\n",
    "定义：以单词为单位进行分词。\n",
    "优点：直观，易于理解。\n",
    "缺点：词表大，难以处理生僻词和新词（OOV 问题）。\n",
    "例子：\n",
    "\"Learning new things is fun!\" → [\"Learning\", \"new\", \"things\", \"is\", \"fun\", \"!\"]\n",
    "+ character-level\n",
    "定义：以单个字符为单位进行分词。\n",
    "优点：词表小，无 OOV 问题。\n",
    "缺点：序列变长，语义信息弱。\n",
    "例子：\n",
    "\"fun\" → [\"f\", \"u\", \"n\"]\n",
    "+ Subword-level（子词级分词）\n",
    "定义：将单词拆分为更小的子词单元（如 BPE、WordPiece、Unigram）。\n",
    "优点：兼顾词表大小和 OOV 问题，能处理新词。\n",
    "缺点：实现复杂。\n",
    "例子：\n",
    "\"prompting\" → [\"prom\", \"pt\", \"ing\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db96a5",
   "metadata": {},
   "source": [
    "1. BPE（Byte Pair Encoding）\n",
    "原理：BPE 最初用于数据压缩，后被用于 NLP 分词。它通过不断合并文本中出现频率最高的字符对，逐步构建子词单元。\n",
    "步骤：\n",
    "+ 将文本拆成单字符序列。\n",
    "+ 统计所有相邻字符对的出现频率。\n",
    "+ 合并出现频率最高的字符对，形成新的子词。\n",
    "+ 重复步骤 2-3，直到达到预设的词表大小或没有高频对可合并。\n",
    "优点：能有效减少 OOV 问题，词表大小可控。\n",
    "应用：GPT、RoBERTa 等模型。\n",
    "2. WordPiece\n",
    "原理：WordPiece 由 Google 提出，最早用于机器翻译，后成为 BERT 的分词方法。与 BPE 类似，但合并策略基于最大化训练数据的似然概率。\n",
    "步骤：\n",
    "+ 初始化词表为所有字符。\n",
    "+ 迭代地选择能最大提升训练数据似然的子词合并。\n",
    "+ 直到词表达到目标大小。\n",
    "优点：能更好地平衡词表大小与覆盖率，适合大规模语料。\n",
    "应用：BERT、ALBERT、DistilBERT 等。\n",
    "3. Unigram Language Model\n",
    "原理：Unigram LM 由 Google 提出，常用于 SentencePiece。它假设每个子词的出现是独立的，通过概率建模选择最优子词集合。\n",
    "步骤：\n",
    "+ 初始化一个大词表（所有可能的子词）。\n",
    "+ 通过 EM 算法估算每个子词的概率。\n",
    "+ 迭代地删除概率最低的子词，直到词表缩小到目标大小。\n",
    "+ 对新词分词时，选择概率乘积最大的子词序列。\n",
    "优点：分词灵活，能自动适应不同语言和文本。\n",
    "应用：SentencePiece（Google）、T5、XLNet 等。\n",
    "\n",
    "面试题与参考答案\n",
    "题目1：请简述 BPE、WordPiece 和 Unigram LM 的主要区别。\n",
    "参考答案：\n",
    "\n",
    "BPE 通过合并高频字符对构建子词，合并策略基于频率。\n",
    "WordPiece 也合并子词，但合并策略基于最大化训练数据的似然概率。\n",
    "Unigram LM 假设子词独立出现，通过概率建模和 EM 算法筛选子词，最终选择概率最大的分词方式。\n",
    "题目2：为什么现代 NLP 模型更倾向于使用子词级分词方法？\n",
    "参考答案：\n",
    "子词级分词方法（如 BPE、WordPiece、Unigram LM）能有效平衡词表大小和 OOV 问题。它们既能处理常见词汇，也能将生僻词拆分为已知子词，提升模型泛化能力和效率。\n",
    "\n",
    "题目3：请简述 BPE 分词的基本流程，并说明其优缺点。\n",
    "参考答案：\n",
    "流程：\n",
    "\n",
    "文本拆为单字符序列。\n",
    "统计所有相邻字符对频率。\n",
    "合并频率最高的字符对，形成新子词。\n",
    "重复直到词表大小满足要求。\n",
    "优点：词表可控，能处理新词。\n",
    "缺点：合并策略简单，未考虑上下文或概率信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dac46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装：pip install tokenizers\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "# 初始化空的 BPE 模型\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# 训练 BPE\n",
    "trainer = trainers.BpeTrainer(vocab_size=1000, min_frequency=2)\n",
    "files = [\"your_text.txt\"]  # 训练语料路径\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# 分词\n",
    "output = tokenizer.encode(\"Learning new things is fun!\")\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d847ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化空的 WordPiece 模型\n",
    "tokenizer = Tokenizer(models.WordPiece())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# 训练 WordPiece\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=1000, min_frequency=2)\n",
    "files = [\"your_text.txt\"]\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# 分词\n",
    "output = tokenizer.encode(\"Learning new things is fun!\")\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装：pip install sentencepiece\n",
    "import sentencepiece as spm\n",
    "\n",
    "# 训练 Unigram 模型\n",
    "spm.SentencePieceTrainer.Train('--input=your_text.txt --model_prefix=unigram --vocab_size=1000 --model_type=unigram')\n",
    "\n",
    "# 加载模型\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('unigram.model')\n",
    "\n",
    "# 分词\n",
    "print(sp.encode('Learning new things is fun!', out_type=str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchCPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
