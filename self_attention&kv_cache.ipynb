{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4fcc3b5",
   "metadata": {},
   "source": [
    "# Self-Attention 与 KV Cache\n",
    "\n",
    "## 1. Self-Attention 机制简介\n",
    "\n",
    "Self-Attention（自注意力）是 Transformer 架构的核心。它允许模型在处理序列时，动态关注序列中不同位置的信息，实现信息的全局交互。\n",
    "\n",
    "### 1.1 原理\n",
    "\n",
    "- 对于输入序列中的每个 token，Self-Attention 会计算它与序列中所有 token 的相关性（注意力分数）。\n",
    "- 通过加权求和的方式，融合其他 token 的信息，生成新的表示。\n",
    "\n",
    "### 1.2 计算流程\n",
    "\n",
    "1. 输入序列经过线性变换，得到 Query（Q）、Key（K）、Value（V）矩阵。\n",
    "2. 计算 Q 与 K 的点积，得到注意力分数（score）。\n",
    "3. 对分数进行 softmax，得到注意力权重。\n",
    "4. 用权重加权 V，得到输出。\n",
    "\n",
    "公式如下：\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "### 1.3 多头注意力\n",
    "\n",
    "- 多头注意力（Multi-Head Attention）将 Q、K、V 分成多组，分别计算注意力，最后拼接结果。\n",
    "- 优点：能捕捉不同子空间的特征。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. KV Cache（键值缓存）\n",
    "\n",
    "### 2.1 背景\n",
    "\n",
    "- 在推理阶段（如生成文本时），Transformer 需要逐步生成下一个 token。\n",
    "- 每次生成时，理论上都要重新计算之前所有 token 的 Self-Attention，效率低下。\n",
    "\n",
    "### 2.2 KV Cache 的作用\n",
    "\n",
    "- **KV Cache** 是一种缓存机制，将之前计算得到的 Key（K）和 Value（V）保存下来。\n",
    "- 新 token 只需计算自己的 Q，并与缓存的 K、V 进行注意力计算，无需重复计算历史部分。\n",
    "\n",
    "### 2.3 优点\n",
    "\n",
    "- 大幅提升推理速度，降低显存和计算消耗。\n",
    "- 是大模型推理（如 ChatGPT、LLM 推理加速）的关键技术。\n",
    "\n",
    "### 2.4 过程\n",
    "在自回归解码（生成文本）过程中，我们的目标是 避免重复计算 K 和 V。KV Cache 采用增量缓存（Incremental Cache）的方式存储 Key 和 Value：\n",
    "\n",
    "第一步（初始计算）：\n",
    "计算第一个 token 的 K_1, V_1，存入缓存。\n",
    "第二步（生成新 token 时）：\n",
    "计算新 token 的 Query Q_n，但 不计算之前 token 的 Key 和 Value，直接从缓存读取 K_1, K_2, ..., K_{n-1} 和 V_1, V_2, ..., V_{n-1}。\n",
    "只计算 Q_nK^T，然后进行注意力计算。\n",
    "最终，存储 K 和 V，仅计算 Q，可以大幅减少计算量：\n",
    "\n",
    "Attention ( Q n , [ K 1 , K 2 , . . . , K n − 1 ] , [ V 1 , V 2 , . . . , V n − 1 ] ) \\text{Attention}(Q_n, [K_1, K_2, ..., K_{n-1}], [V_1, V_2, ..., V_{n-1}])\n",
    "\n",
    "\n",
    "这样，计算复杂度由 O(N²) 降至 O(N)，随着序列长度增长，计算成本大幅降低。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f89bc2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Multi-Head Attention 是顺序/位置不敏感的。**\n",
    "\n",
    "$Q$ 中的每一个元素会和 $K$ 中所有个元素相乘并计算 Attention Score，这个的计算结果和 $K$ 中元素的顺序/位置是没有关系的，只和元素值的大小有关，因此 Multi-Head Attention 是对顺序/位置不敏感的——无论 $Q$ 和 $K$ 中元素的排列顺序如何其对应元素计算的结构都是恒定的、其计算的结果也无法反映其顺序/位置关系。\n",
    "\n",
    "**Positional Encoding 的作用是解决 Multi-Head Attention 的顺序/位置不敏感性。**\n",
    "\n",
    "通过给不同位置的元素加上一个能表示其顺序/位置值，将位置特征反应到了元素的特征值中，使得最终的计算结果是和元素的顺序/位置相关的——让模型利用到数据顺序/位置上的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(Q, K, V, mask=None):\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / d_k ** 0.5\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, V)\n",
    "    return output, attn\n",
    "\n",
    "# KV Cache 示例\n",
    "# 假设历史 Key/Value 已缓存\n",
    "past_K = ...  # [batch, seq_len_past, dim]\n",
    "past_V = ...  # [batch, seq_len_past, dim]\n",
    "new_K = ...   # [batch, 1, dim]\n",
    "new_V = ...   # [batch, 1, dim]\n",
    "\n",
    "# 拼接历史和当前\n",
    "K_cat = torch.cat([past_K, new_K], dim=1)\n",
    "V_cat = torch.cat([past_V, new_V], dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchCPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
