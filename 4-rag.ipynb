{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e913ee",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "第1步：数据准备与知识库构建\n",
    "核心技术\n",
    "\n",
    "多格式解析\n",
    "\n",
    "技术细节：处理PDF、HTML、Word、Markdown等格式，需提取结构化文本并保留标题、列表等逻辑结构。\n",
    "工具推荐：\n",
    "\n",
    "PyPDF2 / PDFMiner：提取PDF文本，解决扫描版PDF需配合OCR（如Tesseract）。\n",
    "BeautifulSoup / Readability：清理网页HTML标签，提取正文。\n",
    "Apache Tika：统一解析多种格式（包括Office文档）。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "文本清洗\n",
    "\n",
    "技术细节：去除广告、导航栏、特殊字符，标准化编码（如UTF-8），处理缩写和拼写错误。\n",
    "正则表达式：匹配删除无关内容（如“点击查看详情”）。\n",
    "NLP工具：Spacy的规则引擎识别保留关键实体（如产品型号、技术术语）。\n",
    "\n",
    "\n",
    "元数据管理\n",
    "\n",
    "技术细节：为文本块附加来源、创建时间、作者等信息，用于后续检索排序。\n",
    "实现方案：在JSON或数据库中存储{text: \"...\", source: \"手册_v3.pdf\", page: 12}。\n",
    "\n",
    "\n",
    "\n",
    "典型代码结构\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber\n",
    "\n",
    "# 解析PDF\n",
    "with pdfplumber.open(\"manual.pdf\") as pdf:\n",
    "    text = \"\\n\".join([page.extract_text() for page in pdf.pages])\n",
    "\n",
    "# 清洗网页\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "main_text = soup.find(\"div\", class_=\"main-content\").get_text(strip=True)\n",
    "\n",
    "\n",
    "第2步：文本切分（Chunking）\n",
    "分块策略\n",
    "\n",
    "固定窗口法：\n",
    "\n",
    "技术细节：每块固定长度（如512 tokens），重叠窗口（overlap=10%）避免语义断裂。\n",
    "代码示例：from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512, chunk_overlap=64, separators=[\"\\n\\n\", \"\\n\", \"。\", \" \"]\n",
    ")\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "语义分块法：\n",
    "\n",
    "技术细节：用NLP模型检测段落边界（如Spacy的句子分割），适合技术文档。\n",
    "工具：Spacy的sentencizer组件或基于Transformer的语义分割模型。\n",
    "\n",
    "\n",
    "\n",
    "优化技巧\n",
    "\n",
    "动态分块：根据内容类型调整块大小（如论文摘要用256 tokens，实验细节用1024 tokens）。\n",
    "分块元数据：标记块类型（标题、代码示例、表格）供检索时加权。\n",
    "\n",
    "\n",
    "第3步：向量化与嵌入（Embedding）\n",
    "嵌入模型选型\n",
    "\n",
    "闭源API：\n",
    "\n",
    "OpenAI text-embedding-3-small：1536维，适合通用场景，成本约$0.0001/1k tokens。\n",
    "Cohere Embed：支持多语言，提供压缩模式（降低维度节省存储）。\n",
    "\n",
    "\n",
    "开源模型：\n",
    "\n",
    "Sentence-BERT（sentence-transformers库）：预训练模型如all-mpnet-base-v2，微调支持领域适配。\n",
    "BGE（BAAI General Embedding）：中文优化，支持指令微调（在提示词前加“为这个句子生成嵌入：”）。\n",
    "\n",
    "\n",
    "\n",
    "批量计算与加速\n",
    "\n",
    "GPU加速：用PyTorch的DataLoader多线程批量处理。\n",
    "降维：PCA或UMAP将向量从1536维降至256维，减少存储和计算量。\n",
    "\n",
    "代码示例\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "embeddings = model.encode(chunks, batch_size=128, show_progress_bar=True)\n",
    "\n",
    "\n",
    "第4步：向量存储与检索\n",
    "数据库选型对比\n",
    "\n",
    "\n",
    "\n",
    "数据库\n",
    "适用场景\n",
    "关键特性\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FAISS\n",
    "中小规模（百万级），本地部署\n",
    "GPU加速、IVF索引、支持L2/IP相似度\n",
    "\n",
    "\n",
    "Pinecone\n",
    "大规模，全托管云服务\n",
    "自动缩放、混合搜索（稀疏+稠密向量）\n",
    "\n",
    "\n",
    "Milvus\n",
    "自托管，分布式扩展\n",
    "支持标量过滤、数据分片、高可用\n",
    "\n",
    "\n",
    "Elasticsearch\n",
    "已有ES生态集成\n",
    "8.0+支持kNN搜索，可与全文检索结合\n",
    "\n",
    "\n",
    "\n",
    "索引结构优化\n",
    "\n",
    "HNSW（Hierarchical Navigable Small World）：\n",
    "\n",
    "参数：efConstruction=200（构建时邻居数）、efSearch=100（搜索时扩展数），权衡精度与速度。\n",
    "\n",
    "\n",
    "IVF（Inverted File Index）：\n",
    "\n",
    "聚类为nlist=4096个分区，搜索时仅查最近簇，适合超大规模数据集。\n",
    "\n",
    "\n",
    "\n",
    "检索逻辑\n",
    "import faiss\n",
    "index = faiss.IndexHNSWFlat(embedding_dim, 32)\n",
    "index.add(embeddings)\n",
    "distances, indices = index.search(query_embedding, k=5)\n",
    "\n",
    "\n",
    "第5步：检索增强生成（RAG）\n",
    "提示词工程\n",
    "\n",
    "模板设计：基于以下上下文，精确回答问题。如果答案不在上下文中，回答“未知”。\n",
    "\n",
    "上下文：\n",
    "{context_str}\n",
    "\n",
    "问题：{query_str}\n",
    "\n",
    "\n",
    "多步推理：对复杂问题先检索多个相关块，分步生成中间结论（Chain-of-Thought）。\n",
    "\n",
    "生成模型优化\n",
    "\n",
    "模型选择：\n",
    "\n",
    "GPT-4：高成本，适合对准确性要求严苛的场景（如医疗、法律）。\n",
    "Llama 3 70B：开源替代，需16GB以上GPU显存，使用vLLM库加速推理。\n",
    "\n",
    "\n",
    "生成参数：\n",
    "\n",
    "temperature=0.3（降低随机性），max_tokens=500，stop_sequences=[\"\\n参考：\"]。\n",
    "\n",
    "\n",
    "\n",
    "代码流程\n",
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=vector_db.as_retriever(),\n",
    "    chain_type=\"stuff\"  # 或\"map_reduce\"处理长文本\n",
    ")\n",
    "answer = qa_chain.run(\"如何拆卸联想Y7000的电池？\")\n",
    "\n",
    "\n",
    "第6步：评估与优化\n",
    "评估指标\n",
    "\n",
    "检索阶段：\n",
    "\n",
    "命中率（Hit Rate）：Top-K结果中包含正确答案的比例。\n",
    "MRR（Mean Reciprocal Rank）：正确答案排名的倒数均值。\n",
    "\n",
    "\n",
    "生成阶段：\n",
    "\n",
    "ROUGE-L：衡量生成文本与标准答案的词汇重叠率。\n",
    "BERTScore：用BERT模型评估语义相似度。\n",
    "\n",
    "\n",
    "\n",
    "优化手段\n",
    "\n",
    "重排序（Rerank）：\n",
    "\n",
    "用交叉编码器（Cross-Encoder）对Top-100检索结果重新评分。\n",
    "代码示例（使用Sentence-Transformers）：from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "scores = model.predict([(query, chunk) for chunk in chunks])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "混合检索：结合关键词（BM25）与语义搜索，提升召回率。\n",
    "\n",
    "\n",
    "扩展：高级RAG架构\n",
    "\n",
    "多跳检索（Multi-Hop）\n",
    "\n",
    "场景：复杂问题需多次检索（如“Y7000的电池型号是什么？该型号的供应商有哪些？”）。\n",
    "实现：用LLM生成中间问题→分步检索→综合答案。\n",
    "\n",
    "\n",
    "动态数据更新\n",
    "\n",
    "技术：监听知识库文件夹变动（如Watchdog库）→触发增量嵌入更新。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "技术栈全景图\n",
    "用户提问 → 解析清洗 → 分块 → 向量化 → 存储 → 检索 → 增强生成 → 评估  \n",
    "                │          │           │         │           │  \n",
    "                ↓          ↓           ↓         ↓           ↓  \n",
    "         Apache Tika   LangChain   Sentence-BERT  FAISS    GPT-4/Llama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef575ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# pip install langchain sentence-transformers faiss-cpu openai tiktoken python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 加载环境变量（用于OpenAI API）\n",
    "load_dotenv() \n",
    "\n",
    "# ========================= 第1步：知识库准备 =========================\n",
    "# 创建示例知识库文件（实际场景替换为你的文档）\n",
    "with open(\"knowledge_base.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\"\"\n",
    "## 联想笔记本电池更换指南\n",
    "1. 关机并断开所有外接电源。\n",
    "2. 使用T5螺丝刀卸下底部螺丝（共8颗）。\n",
    "3. 从边缘撬开底盖，注意不要损坏卡扣。\n",
    "4. 找到电池连接器，轻轻拔出。\n",
    "5. 更换新电池后按相反顺序组装。\n",
    "\n",
    "## 天禧AI隐私保护功能\n",
    "- 本地数据处理：用户对话内容优先在设备端处理。\n",
    "- 差分隐私技术：上传数据时添加噪声保护个体信息。\n",
    "- 权限控制：可随时在设置中关闭数据收集。\n",
    "    \"\"\")\n",
    "\n",
    "# ========================= 第2步：文档加载与分块 =========================\n",
    "loader = TextLoader(\"knowledge_base.md\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 智能分块（每块512字符，重叠64字符）\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=64,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"。\", \" \"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# ========================= 第3步：向量嵌入与存储 =========================\n",
    "# 使用Sentence-BERT生成嵌入\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# 创建FAISS向量库\n",
    "vector_db = FAISS.from_documents(\n",
    "    chunks, \n",
    "    embed_model\n",
    ")\n",
    "vector_db.save_local(\"faiss_index\")  # 保存索引供后续使用\n",
    "\n",
    "# ========================= 第4步：检索增强生成 =========================\n",
    "def rag_answer(question: str) -> str:\n",
    "    # 加载预存的向量库\n",
    "    vector_db = FAISS.load_local(\n",
    "        \"faiss_index\", \n",
    "        embed_model,\n",
    "        allow_dangerous_deserialization=True  # FAISS需要此参数\n",
    "    )\n",
    "    \n",
    "    # 检索最相关的3个块\n",
    "    retrieved_docs = vector_db.similarity_search(question, k=3)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # 构建提示词模板\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"基于以下上下文，用简洁的技术语言回答问题。如果无法回答，请说明未知内容。\\n\\n上下文：{context}\"),\n",
    "        (\"human\", \"问题：{question}\")\n",
    "    ])\n",
    "    \n",
    "    # 调用生成模型（此处用OpenAI，生产环境可替换为Llama等）\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.3\n",
    "    )\n",
    "    chain = prompt_template | llm\n",
    "    \n",
    "    # 执行生成\n",
    "    response = chain.invoke({\"question\": question, \"context\": context})\n",
    "    return response.content\n",
    "\n",
    "# ========================= 测试 =========================\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        question = input(\"\\n请输入问题（输入q退出）: \")\n",
    "        if question.lower() == \"q\":\n",
    "            break\n",
    "        print(\"\\n回答：\", rag_answer(question))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
