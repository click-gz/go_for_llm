{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816a0ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeb7f7e7",
   "metadata": {},
   "source": [
    "llama3改进：\n",
    "1. tokenizer\n",
    "使用tiktoken， tiktoken 使用 BPE 算法，能有效处理多语言和特殊字符，减少 OOV 问题。而不是直接使用空格分词；词表更大；优化特殊token。\n",
    "2. GQA\n",
    "query为每个头独立生成，key和value只为部分头生成（分组生成）\n",
    "在传统的多头自注意力（Multi-Head Attention）中，Query、Key、Value 都是为每个头独立生成的，计算量和显存消耗都很大。\n",
    "GQA 通过减少 Key/Value 的数量，显著降低了推理和训练的资源消耗，尤其在大模型（数十亿参数）中效果明显。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db0f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: [batch, seq, num_heads, head_dim]\n",
    "# K/V: [batch, seq, num_kv_heads, head_dim]\n",
    "Q = linear_q(x)  # num_heads\n",
    "K = linear_k(x)  # num_kv_heads\n",
    "V = linear_v(x)  # num_kv_heads\n",
    "\n",
    "# 将 Q 头分组，每组共享一组 K/V\n",
    "Q = Q.reshape(batch, seq, num_groups, heads_per_group, head_dim)\n",
    "K = K.reshape(batch, seq, num_groups, head_dim)\n",
    "V = V.reshape(batch, seq, num_groups, head_dim)\n",
    "\n",
    "# 计算注意力\n",
    "attn_scores = Q @ K.transpose(-1, -2) / sqrt(head_dim)\n",
    "attn_probs = softmax(attn_scores)\n",
    "output = attn_probs @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aada49f",
   "metadata": {},
   "source": [
    "3. RMSNorm\n",
    "Layer Normalization） 是一种替代 LayerNorm 的归一化方法，常用于 Llama3 等大模型。它的主要特点是只对输入的均方根（RMS）进行归一化，不涉及均值和偏置，参数更少，计算更高效。\n",
    "假设输入为 $x \\in \\mathbb{R}^d$，$\\epsilon$ 是一个很小的常数防止除零，$\\gamma$ 是可学习的缩放参数：\n",
    "\n",
    "$$ \\text{RMSNorm}(x) = \\frac{x}{\\text{RMS}(x) + \\epsilon} \\cdot \\gamma $$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$ \\text{RMS}(x) = \\sqrt{\\frac{1}{d} \\sum_{i=1}^d x_i^2} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3060281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, ..., dim]\n",
    "        rms = x.pow(2).mean(-1, keepdim=True).sqrt()\n",
    "        x_norm = x / (rms + self.eps)\n",
    "        return x_norm * self.weight\n",
    "\"\"\"\n",
    "避免均值漂移：LayerNorm 归一化时会减去均值，可能导致数值漂移，\n",
    "RMSNorm 只缩放不平移，数值更稳定。\n",
    "梯度更平滑：RMSNorm 的归一化方式对梯度影响较小，训练大模型时更容易收敛，\n",
    "表现出更好的稳定性。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ffbc2",
   "metadata": {},
   "source": [
    "4. ROPE\n",
    "RoPE 通过将每个 token 的 Query 和 Key 向量在特定维度上进行“旋转”，把位置信息直接编码进注意力计算中。\n",
    "这种方式不需要额外的可学习参数，也不需要将位置编码与输入相加，而是通过数学变换实现。\n",
    "对于每个 token 的向量 $x$，RoPE 将其分为偶数和奇数维度，然后用如下方式编码第 $i$ 个 token 的位置 $p$：\n",
    "\n",
    "$$ \\text{RoPE}(x, p) = x_{2k} \\cdot \\cos(\\theta_p) + x_{2k+1} \\cdot \\sin(\\theta_p) \\ \\text{RoPE}(x, p) = -x_{2k} \\cdot \\sin(\\theta_p) + x_{2k+1} \\cdot \\cos(\\theta_p) $$\n",
    "\n",
    "其中 $\\theta_p$ 是与位置 $p$ 和维度 $k$ 相关的角度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33184808",
   "metadata": {},
   "source": [
    "5. KV cache\n",
    "首次输入：模型对输入序列计算所有 Key/Value，并存入缓存。\n",
    "生成新 token：只需对新 token 计算 Key/Value，然后与缓存拼接，参与注意力计算。\n",
    "下次生成：继续复用缓存，直到生成结束。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d873a03",
   "metadata": {},
   "source": [
    "6. SwiGLU\n",
    "SwiGLU 结合了门控机制和 Swish 激活，具体结构如下：\n",
    "\n",
    "$$ \\text{SwiGLU}(x) = (xW_1) \\odot \\text{Swish}(xW_2) $$\n",
    "\n",
    "其中：\n",
    "\n",
    "$x$ 是输入向量\n",
    "$W_1, W_2$ 是线性变换权重\n",
    "$\\odot$ 表示逐元素乘法\n",
    "$\\text{Swish}(z) = z \\cdot \\sigma(z)$，$\\sigma$ 是 Sigmoid 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53492033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear1(x) * F.silu(self.linear2(x))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
